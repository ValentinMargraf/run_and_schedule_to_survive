{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats.mstats import rankdata\n",
    "from aslib_scenario.aslib_scenario import ASlibScenario\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "### Matplotlib settings ###\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "font = {'family' : 'normal',\n",
    "        'size'   : 16}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "#rcParams.update({'figure.autolayout': True})\n",
    "rcParams.update(mpl.rcParamsDefault)\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context(font_scale=5.0, rc={\"lines.linewidth\": 3.0})\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for normalized PAR10 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survival_data = pd.read_csv(r'./clipped_survival_normalized_par10.csv')\n",
    "# survival_data.columns = ['ASlib Scenarios', 'Algorithm Selector', 'metric', 'normalized', 'num_results']\n",
    "\n",
    "# name_map = {'ExpectationSurvivalForest':'Run2SurviveExp', 'RiskAverseSurvivalForest':'Run2SurvivePoly/Log', 'PAR10SurvivalForest':'Run2SurvivePAR10'}\n",
    "# survival_data = survival_data.replace(name_map)\n",
    "# survival_data = survival_data[survival_data['Algorithm Selector'] !='GridSearchSurvivalForest']\n",
    "# survival_data.to_csv(r'./run2survive_normalized_par10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For each baseline select results from the overall best setting (PAR10, Runtime or Ignored) in terms of median performance aggregated over all scenarios \n",
    "# get data for own approaches from normalize par10 table\n",
    "survival_data = pd.read_csv(r'./run2survive_normalized_par10.csv')\n",
    "survival_data.columns = ['ASlib Scenarios', 'Algorithm Selector', 'metric', 'normalized', 'num_results']\n",
    "#num_approaches = data['Algorithm Selector'].nunique()\n",
    "survival_data = survival_data.pivot('ASlib Scenarios','Algorithm Selector','normalized')\n",
    "survival_data = survival_data[['Run2SurviveExp', 'Run2SurvivePAR10', 'Run2SurvivePoly/Log']]\n",
    "\n",
    "# get data for baselines from the comparison par10 table (best value from different settings)\n",
    "baseline_data =  pd.read_csv(r'./setting_comparison_normalized_par10.csv')\n",
    "min_median_setting = baseline_data.groupby(['Algorithm Selector', 'Metric']).median().idxmin(axis=1)\n",
    "min_median_setting = min_median_setting.droplevel(level=1)\n",
    "min_median_setting = list(zip(min_median_setting.index, min_median_setting))\n",
    "baseline_map = dict(min_median_setting).copy()\n",
    "baseline_data = baseline_data.pivot('ASlib Scenario', 'Algorithm Selector', ['PAR10', 'Runtime', 'Ignored']).swaplevel(axis=1).sort_index(1)\n",
    "baseline_data = baseline_data[min_median_setting]\n",
    "baseline_data = baseline_data.droplevel(axis=1, level=1)\n",
    "\n",
    "# add Schmee & Hahn for PerAlgorithmRegressor and recompute min median\n",
    "schmee_data = pd.read_csv(r'./schmee_normalized_par10.csv')\n",
    "schmee_data = schmee_data[schmee_data['Algorithm Selector'] == 'ImputedPerAlgorithmRegressor']\n",
    "schmee_data = schmee_data.set_index(keys='ASlib Scenario', drop=True)\n",
    "if schmee_data['normalized'].median() < baseline_data['PerAlgorithmRegressor'].median():\n",
    "    baseline_data['PerAlgorithmRegressor'] = schmee_data['normalized']\n",
    "    baseline_map['PerAlgorithmRegressor'] = 'S & H'\n",
    "    \n",
    "# adjust ordering of algorithms\n",
    "baseline_data = baseline_data[['PerAlgorithmRegressor', 'MultiClassSelector', 'ISAC*like', 'SATzilla\\'11*like', 'SUNNY*like']]\n",
    "\n",
    "data = pd.concat([survival_data, baseline_data], axis = 1).T.drop_duplicates().T\n",
    "baseline_index = [''.join([selector, '\\n(' + baseline_map[selector] + ')']) for selector in data.columns if selector in baseline_map.keys()]\n",
    "baseline_index = [selector  if not selector.startswith('MultiClassSelector') else 'MultiClassSelector' for selector in baseline_index]\n",
    "data = data.rename(columns={selector: ''.join([selector, '\\n(' + setting + ')']) for selector, setting in baseline_map.items() if selector != 'MultiClassSelector'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap for normalized PAR10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap for Survival Forest vs Baselines comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_algorithms = ['Run2SurviveExp', 'Run2SurvivePAR10', 'Run2SurvivePoly/Log']\n",
    "baselines = ['PerAlgorithmRegressor\\n(Runtime)', 'MultiClassSelector',       'ISAC*like\\n(PAR10)', 'SATzilla\\'11*like\\n(Runtime)','SUNNY*like\\n(Runtime)']\n",
    "approaches = survival_algorithms + baselines\n",
    "num_survival_algorithms = len(survival_algorithms)\n",
    "num_baselines = len(baselines)\n",
    "num_scenarios, num_algorithms = data.values.shape\n",
    "\n",
    "fig = plt.figure(figsize=(10, 12), constrained_layout=False)\n",
    "gs = gridspec.GridSpec(4, 3, width_ratios=[30, 50, 2.5], height_ratios=[125, 5, 5, 5], figure=fig, wspace=0.05, hspace=0.03)\n",
    "ax1_1 = fig.add_subplot(gs[0, 0])\n",
    "ax1_2 = fig.add_subplot(gs[0, 1])\n",
    "ax1_3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "ax2_1 = fig.add_subplot(gs[1, 0])\n",
    "ax2_2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "ax3_1 = fig.add_subplot(gs[2, 0])\n",
    "ax3_2 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "ax4_1 = fig.add_subplot(gs[3, 0])\n",
    "ax4_2 = fig.add_subplot(gs[3, 1])\n",
    "\n",
    "normal_vmax = 1.5\n",
    "palette = sns.color_palette(['#FAFAFA', '#F2F2F2', '#EBEBEB', '#E3E3E3', '#DBDBDB', '#D6D6D6', '#D4D4D4', '#CCCCCC', '#C7C7C7', '#BFBFBF', '#BDBDBD', '#B5B5B5', '#ADADAD', '#A9A9A9', '#A6A6A6', '#9E9E9E', '#969696', '#8F8F8F', '#878787', '#808080'], n_colors=20)\n",
    "\n",
    "\n",
    "tmp_data = data[survival_algorithms]\n",
    "heatmap = sns.heatmap(tmp_data, cmap=palette, linewidths=.5, ax=ax1_1, annot=True, cbar_kws={'label': 'Normalized PAR10'}, vmax=normal_vmax, vmin=0.0, cbar=False)\n",
    "ax1_1.tick_params(axis='both', which='major', labelsize=10, labelbottom=False, bottom=False, top=False, labeltop=True)\n",
    "labels = ax1_1.get_xticklabels()\n",
    "ax1_1.set_xticklabels(labels, rotation=45, ha='left')\n",
    "ax1_1.set_xlabel('')\n",
    "ax1_1.set_ylabel('')\n",
    "\n",
    "\n",
    "tmp_data = data[baseline_index]\n",
    "heatmap = sns.heatmap(tmp_data, cmap=palette, linewidths=.5, ax=ax1_2, annot=True, cbar_kws={'label': 'Normalized PAR10'}, vmax=normal_vmax, vmin=0.0, cbar_ax=ax1_3)\n",
    "\n",
    "ax1_2.tick_params(axis='both', which='major', labelsize=10, labelbottom=False, bottom=False, top=False, labeltop=True)\n",
    "ax1_2.axes.get_yaxis().set_visible(False)\n",
    "labels = ax1_2.get_xticklabels()\n",
    "ax1_2.set_xticklabels(labels, rotation=45, ha='left')\n",
    "ax1_2.set_xlabel('')\n",
    "ax1_2.set_ylabel('')\n",
    "\n",
    "\n",
    "\n",
    "for scenario_id in range(num_scenarios):\n",
    "    tmp_1 = ax1_1.texts[scenario_id * num_survival_algorithms: scenario_id * num_survival_algorithms + num_survival_algorithms]    \n",
    "    tmp_2 = ax1_2.texts[scenario_id * num_baselines: scenario_id * num_baselines + num_baselines]\n",
    "    min_val = min(min([float(text.get_text()) for text in tmp_1]), min([float(text.get_text()) for text in tmp_2]))\n",
    "    min_baseline_val = min([float(text.get_text()) for text in tmp_2])\n",
    "    indices_1_underline = [num for num, text in enumerate(tmp_1) if float(text.get_text()) <= min_baseline_val]\n",
    "    indices_1 = [num for num, text in enumerate(tmp_1) if float(text.get_text()) == min_val]\n",
    "    indices_2 = [num for num, text in enumerate(tmp_2) if float(text.get_text()) == min_val]\n",
    "    \n",
    "    for idx in set(indices_1_underline).intersection(set(indices_1)):\n",
    "        text = tmp_1[idx]\n",
    "        text.set_text(r'$\\mathbf{\\overline{%s}}$' % text.get_text())\n",
    "        text.set_size(11)\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')\n",
    "        \n",
    "    for idx in set(indices_1_underline).difference(set(indices_1)):\n",
    "        text = tmp_1[idx]\n",
    "        text.set_text(r'$\\overline{%s}$' % text.get_text())\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')\n",
    "        \n",
    "                \n",
    "    for idx in indices_2:\n",
    "        text = tmp_2[idx]\n",
    "        #text.set_size(14)\n",
    "        text.set_weight('bold')\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')     \n",
    "\n",
    "bottom_palette =sns.color_palette(\"Blues\", n_colors=100)\n",
    "\n",
    "ax2_data = np.median(data[approaches].values, axis=0)\n",
    "ax2_data = ax2_data.reshape(1, ax2_data.size)\n",
    "ax2_vmin, ax2_vmax = np.nanmin(ax2_data), np.nanmax(ax2_data)\n",
    "sns.heatmap(ax2_data[:, :num_survival_algorithms], ax=ax2_1, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax2_vmin, vmax=ax2_vmax)\n",
    "ax2_1.set_ylabel('Median', rotation=0, labelpad=35, y=-0.05)\n",
    "sns.heatmap(ax2_data[:, num_survival_algorithms:], ax=ax2_2, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax2_vmin, vmax=ax2_vmax)\n",
    "ax2_2.set_ylabel('')\n",
    "\n",
    "min_val = min(min([float(text.get_text()) for text in ax2_1.texts]), min([float(text.get_text()) for text in ax2_2.texts]))\n",
    "indices_1 = [num for num, text in enumerate(ax2_1.texts) if float(text.get_text()) == min_val]\n",
    "indices_2 = [num for num, text in enumerate(ax2_2.texts) if float(text.get_text()) == min_val]\n",
    "\n",
    "min_baseline_val = min([float(text.get_text()) for text in ax2_2.texts])\n",
    "indices_1_underline = [num for num, text in enumerate(ax2_1.texts) if float(text.get_text()) <= min_baseline_val]\n",
    "\n",
    "for idx in set(indices_1_underline).intersection(set(indices_1)):\n",
    "    text = ax2_1.texts[idx]\n",
    "    text.set_text(r'$\\mathbf{\\overline{%s}}$' % text.get_text())\n",
    "    text.set_size(11)\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "for idx in set(indices_1_underline).difference(set(indices_1)):\n",
    "    text = ax2_1.texts[idx]\n",
    "    text.set_text(r'$\\overline{%s}$' % text.get_text())\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "for idx in indices_2:\n",
    "    text = ax2_2.texts[idx]\n",
    "    #text.set_size(14)\n",
    "    text.set_weight('bold')\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "\n",
    "ax3_data = np.mean(data[approaches].values, axis=0)\n",
    "ax3_data = ax3_data.reshape(1, ax3_data.size)\n",
    "ax3_vmin, ax3_vmax = np.nanmin(ax3_data), np.nanmax(ax3_data)\n",
    "sns.heatmap(ax3_data[:, :num_survival_algorithms], ax=ax3_1, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax3_vmin, vmax=ax3_vmax)\n",
    "ax3_1.set_ylabel('Mean', rotation=0, labelpad=35, y=-0.05)\n",
    "sns.heatmap(ax3_data[:, num_survival_algorithms:], ax=ax3_2, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax3_vmin, vmax=ax3_vmax)\n",
    "ax3_2.set_ylabel('')\n",
    "\n",
    "\n",
    "min_val = min(min([float(text.get_text()) for text in ax3_1.texts]), min([float(text.get_text()) for text in ax3_2.texts]))\n",
    "indices_1 = [num for num, text in enumerate(ax3_1.texts) if float(text.get_text()) == min_val]\n",
    "indices_2 = [num for num, text in enumerate(ax3_2.texts) if float(text.get_text()) == min_val]\n",
    "\n",
    "min_baseline_val = min([float(text.get_text()) for text in ax3_2.texts])\n",
    "indices_1_underline = [num for num, text in enumerate(ax3_1.texts) if float(text.get_text()) <= min_baseline_val]\n",
    "\n",
    "for idx in set(indices_1_underline).intersection(set(indices_1)):\n",
    "    text = ax3_1.texts[idx]\n",
    "    text.set_text(r'$\\mathbf{\\overline{%s}}$' % text.get_text())\n",
    "    text.set_size(11)\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "for idx in set(indices_1_underline).difference(set(indices_1)):\n",
    "    text = ax3_1.texts[idx]\n",
    "    text.set_text(r'$\\overline{%s}$' % text.get_text())\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "for idx in indices_2:\n",
    "    text = ax3_2.texts[idx]\n",
    "    #text.set_size(14)\n",
    "    text.set_weight('bold')\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "\n",
    "ax4_data = rankdata(data[approaches].values, axis=1)\n",
    "ax4_data = np.mean(ax4_data, axis=0)\n",
    "ax4_data = ax4_data.reshape(1, ax4_data.size)\n",
    "ax4_vmin, ax4_vmax = 1, np.size(ax4_data, axis=1)\n",
    "sns.heatmap(ax4_data[:, :num_survival_algorithms], ax=ax4_1, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax4_vmin, vmax=ax4_vmax)\n",
    "ax4_1.set_ylabel('Mean Rank', rotation=0, labelpad=45, y=-0.05)\n",
    "sns.heatmap(ax4_data[:, num_survival_algorithms:], ax=ax4_2, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax4_vmin, vmax=ax4_vmax)\n",
    "ax4_2.set_ylabel('')\n",
    "\n",
    "min_val = min(min([float(text.get_text()) for text in ax4_1.texts]), min([float(text.get_text()) for text in ax4_2.texts]))\n",
    "indices_1 = [num for num, text in enumerate(ax4_1.texts) if float(text.get_text()) == min_val]\n",
    "indices_2 = [num for num, text in enumerate(ax4_2.texts) if float(text.get_text()) == min_val]\n",
    "\n",
    "min_baseline_val = min([float(text.get_text()) for text in ax4_2.texts])\n",
    "indices_1_underline = [num for num, text in enumerate(ax4_1.texts) if float(text.get_text()) <= min_baseline_val]\n",
    "\n",
    "for idx in set(indices_1_underline).intersection(set(indices_1)):\n",
    "    text = ax4_1.texts[idx]\n",
    "    text.set_text(r'$\\mathbf{\\overline{%s}}$' % text.get_text())\n",
    "    text.set_size(11)\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "for idx in set(indices_1_underline).difference(set(indices_1)):\n",
    "    text = ax4_1.texts[idx]\n",
    "    text.set_text(r'$\\overline{%s}$' % text.get_text())\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "for idx in indices_2:\n",
    "    text = ax4_2.texts[idx]\n",
    "    #text.set_size(14)\n",
    "    text.set_weight('bold')\n",
    "    text.set_color('black')\n",
    "    text.set_style('italic')\n",
    "\n",
    "\n",
    "fig.savefig(r'./run2survive_normalized_par10_heatmap.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Separate heatmaps for normalized PAR10 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# survival_forest_list = ['ExpectationSurvivalForest', 'AutoSurvivalForest', 'GridSearchSurvivalForest']\n",
    "# print(data)\n",
    "\n",
    "# DEBUG\n",
    "survival_forest_list = ['Run2SurviveExp', 'Run2SurvivePAR10', 'Run2SurvivePoly/Log']\n",
    "# DEBUG\n",
    "\n",
    "baselines = ['PerAlgorithmRegressor\\n(Runtime)', 'MultiClassSelector', 'ISAC*like\\n(PAR10)', 'SATzilla\\'11*like\\n(Runtime)', 'SUNNY*like\\n(Runtime)']\n",
    "num_survival_approaches = len(survival_forest_list)\n",
    "\n",
    "for survival_forest in survival_forest_list:\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 10), constrained_layout=False)\n",
    "    gs = gridspec.GridSpec(4, 3, width_ratios=[10, 50, 2.5], height_ratios=[125, 5, 5, 5], figure=fig, wspace=0.05, hspace=0.03)\n",
    "    ax1_1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1_2 = fig.add_subplot(gs[0, 1])\n",
    "    ax1_3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "    ax2_1 = fig.add_subplot(gs[1, 0])\n",
    "    ax2_2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    ax3_1 = fig.add_subplot(gs[2, 0])\n",
    "    ax3_2 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "    ax4_1 = fig.add_subplot(gs[3, 0])\n",
    "    ax4_2 = fig.add_subplot(gs[3, 1])\n",
    "\n",
    "    normal_vmax = 1.5\n",
    "    palette = sns.color_palette(['#FAFAFA', '#F2F2F2', '#EBEBEB', '#E3E3E3', '#DBDBDB', '#D6D6D6', '#D4D4D4', '#CCCCCC', '#C7C7C7', '#BFBFBF', '#BDBDBD', '#B5B5B5', '#ADADAD', '#A9A9A9', '#A6A6A6', '#9E9E9E', '#969696', '#8F8F8F', '#878787', '#808080'], n_colors=20)\n",
    "\n",
    "\n",
    "    tmp_data = data[[survival_forest]]\n",
    "    heatmap = sns.heatmap(tmp_data, cmap=palette, linewidths=.5, ax=ax1_1, annot=True, cbar_kws={'label': 'Normalized PAR10'}, vmax=normal_vmax, vmin=0.0, cbar=False)\n",
    "    ax1_1.tick_params(axis='both', which='major', labelsize=10, labelbottom=False, bottom=False, top=False, labeltop=True)\n",
    "    labels = ax1_1.get_xticklabels()\n",
    "    ax1_1.set_xticklabels(labels, rotation=45, ha='left')\n",
    "    ax1_1.set_xlabel('')\n",
    "    ax1_1.set_ylabel('')\n",
    "\n",
    "\n",
    "    tmp_data = data[baseline_index]\n",
    "    heatmap = sns.heatmap(tmp_data, cmap=palette, linewidths=.5, ax=ax1_2, annot=True, cbar_kws={'label': 'Normalized PAR10'}, vmax=normal_vmax, vmin=0.0, cbar_ax=ax1_3)\n",
    "\n",
    "    ax1_2.tick_params(axis='both', which='major', labelsize=10, labelbottom=False, bottom=False, top=False, labeltop=True)\n",
    "    ax1_2.axes.get_yaxis().set_visible(False)\n",
    "    labels = ax1_2.get_xticklabels()\n",
    "    ax1_2.set_xticklabels(labels, rotation=45, ha='left')\n",
    "    ax1_2.set_xlabel('')\n",
    "    ax1_2.set_ylabel('')\n",
    "\n",
    "\n",
    "    # WORAROUND TO A BUG IN MATPLOTLIB 3.1.1 WHERE THE FIRST AND LAST ROW OF THE HEATMAP ARE CUT\n",
    "    #bottom, top = ax1_1.get_ylim()\n",
    "    #ax1_1.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    #bottom, top = ax1_2.get_ylim()\n",
    "    #ax1_2.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "\n",
    "    num_scenarios, num_algorithms = data.values.shape\n",
    "    for scenario_id in range(num_scenarios):\n",
    "        tmp_1 = ax1_1.texts[scenario_id * 1: scenario_id * 1 + 1]    \n",
    "        tmp_2 = ax1_2.texts[scenario_id * 5: scenario_id * 5 + 5]\n",
    "\n",
    "        ### DEBUG ###\n",
    "        a = [float(text.get_text()) for text in tmp_1]\n",
    "        # print(a)\n",
    "\n",
    "        b = [float(text.get_text()) for text in tmp_2]\n",
    "        # print(b)\n",
    "\n",
    "\n",
    "        min_val = min(min([float(text.get_text()) for text in tmp_1]), min([float(text.get_text()) for text in tmp_2]))\n",
    "        ### DEBUG ###\n",
    "\n",
    "\n",
    "\n",
    "        indices_1 = [num for num, text in enumerate(tmp_1) if float(text.get_text()) == min_val]\n",
    "        indices_2 = [num for num, text in enumerate(tmp_2) if float(text.get_text()) == min_val]\n",
    "\n",
    "        for idx in indices_1:\n",
    "            text = tmp_1[idx]\n",
    "            #text.set_size(14)\n",
    "            text.set_weight('bold')\n",
    "            text.set_color('black')\n",
    "            text.set_style('italic')\n",
    "\n",
    "        for idx in indices_2:\n",
    "            text = tmp_2[idx]\n",
    "            #text.set_size(14)\n",
    "            text.set_weight('bold')\n",
    "            text.set_color('black')\n",
    "            text.set_style('italic')     \n",
    "\n",
    "    bottom_palette =sns.color_palette(\"Blues\", n_colors=100)\n",
    "    tmp_cols = [survival_forest] + baselines\n",
    "    tmp_data = data[tmp_cols]\n",
    "    \n",
    "    \n",
    "    ax2_data = np.median(tmp_data.values, axis=0)\n",
    "    ax2_data = ax2_data.reshape(1, ax2_data.size)\n",
    "\n",
    "    ax2_vmin, ax2_vmax = np.nanmin(ax2_data), np.nanmax(ax2_data)\n",
    "    sns.heatmap(ax2_data[:, :1], ax=ax2_1, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax2_vmin, vmax=ax2_vmax)\n",
    "    ax2_1.set_ylabel('Median', rotation=0, labelpad=25)\n",
    "    sns.heatmap(ax2_data[:, 1:], ax=ax2_2, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax2_vmin, vmax=ax2_vmax)\n",
    "    ax2_2.set_ylabel('')\n",
    "\n",
    "    min_val = min(min([float(text.get_text()) for text in ax2_1.texts]), min([float(text.get_text()) for text in ax2_2.texts]))\n",
    "    indices_1 = [num for num, text in enumerate(ax2_1.texts) if float(text.get_text()) == min_val]\n",
    "    indices_2 = [num for num, text in enumerate(ax2_2.texts) if float(text.get_text()) == min_val]\n",
    "\n",
    "    for idx in indices_1:\n",
    "        text = ax2_1.texts[idx]\n",
    "        #text.set_size(14)\n",
    "        text.set_weight('bold')\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')\n",
    "\n",
    "    for idx in indices_2:\n",
    "        text = ax2_2.texts[idx]\n",
    "        #text.set_size(14)\n",
    "        text.set_weight('bold')\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')\n",
    "\n",
    "\n",
    "    ax3_data = np.mean(tmp_data.values, axis=0)\n",
    "    ax3_data = ax3_data.reshape(1, ax3_data.size)\n",
    "    ax3_vmin, ax3_vmax = np.nanmin(ax3_data), np.nanmax(ax3_data)\n",
    "    sns.heatmap(ax3_data[:, :1], ax=ax3_1, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax3_vmin, vmax=ax3_vmax)\n",
    "    ax3_1.set_ylabel('Mean', rotation=0, labelpad=25)\n",
    "    sns.heatmap(ax3_data[:, 1:], ax=ax3_2, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax3_vmin, vmax=ax3_vmax)\n",
    "    ax3_2.set_ylabel('')\n",
    "\n",
    "\n",
    "    min_val = min(min([float(text.get_text()) for text in ax3_1.texts]), min([float(text.get_text()) for text in ax3_2.texts]))\n",
    "    indices_1 = [num for num, text in enumerate(ax3_1.texts) if float(text.get_text()) == min_val]\n",
    "    indices_2 = [num for num, text in enumerate(ax3_2.texts) if float(text.get_text()) == min_val]\n",
    "\n",
    "    for idx in indices_1:\n",
    "        text = ax3_1.texts[idx]\n",
    "        #text.set_size(14)\n",
    "        text.set_weight('bold')\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')\n",
    "\n",
    "    for idx in indices_2:\n",
    "        text = ax3_2.texts[idx]\n",
    "        #text.set_size(14)\n",
    "        text.set_weight('bold')\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')\n",
    "\n",
    "    ax4_data = rankdata(tmp_data.values, axis=1)\n",
    "    ax4_data = np.mean(ax4_data, axis=0)\n",
    "    ax4_data = ax4_data.reshape(1, ax4_data.size)\n",
    "    ax4_vmin, ax4_vmax = 1, np.size(ax4_data, axis=1)\n",
    "    sns.heatmap(ax4_data[:, :1], ax=ax4_1, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax4_vmin, vmax=ax4_vmax)\n",
    "    ax4_1.set_ylabel('Mean Rank', rotation=0, labelpad=25)\n",
    "    sns.heatmap(ax4_data[:, 1:], ax=ax4_2, annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=ax4_vmin, vmax=ax4_vmax)\n",
    "    ax4_2.set_ylabel('')\n",
    "\n",
    "\n",
    "    min_val = min(min([float(text.get_text()) for text in ax4_1.texts]), min([float(text.get_text()) for text in ax4_2.texts]))\n",
    "    indices_1 = [num for num, text in enumerate(ax4_1.texts) if float(text.get_text()) == min_val]\n",
    "    indices_2 = [num for num, text in enumerate(ax4_2.texts) if float(text.get_text()) == min_val]\n",
    "\n",
    "    for idx in indices_1:\n",
    "        text = ax4_1.texts[idx]\n",
    "        #text.set_size(14)\n",
    "        text.set_weight('bold')\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')\n",
    "\n",
    "    for idx in indices_2:\n",
    "        text = ax4_2.texts[idx]\n",
    "        #text.set_size(14)\n",
    "        text.set_weight('bold')\n",
    "        text.set_color('black')\n",
    "        text.set_style('italic')\n",
    "        \n",
    "    fig.savefig(r'./heatmap_' + survival_forest[:-4] + '.pdf', bbox_inches='tight')  #### DEBUG: Delete [:-4]\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Survival Forest on QBF-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load ASlib Scenario #####\n",
    "scenario_name = 'QBF-2011'\n",
    "fold = 1\n",
    "scenario = ASlibScenario()\n",
    "\n",
    "\n",
    "scenario.read_scenario(Path('./workspaces/aslib/' + scenario_name))\n",
    "test_scenario, train_scenario = scenario.get_split(indx=fold)\n",
    "num_algorithms = len(train_scenario.algorithms)\n",
    "num_instances = train_scenario.instances\n",
    "algorithm_cutoff_time = train_scenario.algorithm_cutoff_time\n",
    "features = train_scenario.feature_data.to_numpy()\n",
    "performances = train_scenario.performance_data.to_numpy()\n",
    "\n",
    "\n",
    "##### Fit Random Survival Forest #####\n",
    "def construct_dataset_for_algorithm_id(instance_features, performances, algorithm_id: int,\n",
    "                                           algorithm_cutoff_time):\n",
    "    # get runtimes of algorithm \n",
    "    performances_of_algorithm_with_id = performances.iloc[:, algorithm_id].to_numpy() if isinstance(performances, pd.DataFrame) else performances[:, algorithm_id]\n",
    "    num_instances = len(performances_of_algorithm_with_id)\n",
    "\n",
    "    # for each instance determine whether it was finished before cutoff; also set PAR10 values\n",
    "    finished_before_timeout = np.empty(num_instances, dtype=bool)\n",
    "    for i in range(0, len(performances_of_algorithm_with_id)):\n",
    "        finished_before_timeout[i] = True if (performances_of_algorithm_with_id[i] < algorithm_cutoff_time) else False\n",
    "        if performances_of_algorithm_with_id[i] >= algorithm_cutoff_time:\n",
    "            performances_of_algorithm_with_id[i] = (algorithm_cutoff_time * 10)\n",
    "\n",
    "    # for each instance build target, consisting of (censored, runtime)\n",
    "    status_and_performance_of_algorithm_with_id = np.empty(dtype=[('cens', np.bool), ('time', np.float)],\n",
    "                                                           shape=instance_features.shape[0])\n",
    "    status_and_performance_of_algorithm_with_id['cens'] = finished_before_timeout\n",
    "    status_and_performance_of_algorithm_with_id['time'] = performances_of_algorithm_with_id\n",
    "\n",
    "    if isinstance(instance_features, pd.DataFrame):\n",
    "        instance_features = instance_features.to_numpy()\n",
    "\n",
    "    return instance_features, status_and_performance_of_algorithm_with_id.T\n",
    "\n",
    "\n",
    "imputer = [SimpleImputer() for _ in range(num_algorithms)]\n",
    "scaler = [StandardScaler() for _ in range(num_algorithms)]\n",
    "params = {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0, 'max_features': 'sqrt', 'bootstrap': True, 'oob_score': False}\n",
    "models = [RandomSurvivalForest(n_estimators=params['n_estimators'],\n",
    "                         min_samples_split=params['min_samples_split'],\n",
    "                         min_samples_leaf=params['min_samples_leaf'],\n",
    "                         min_weight_fraction_leaf = params['min_weight_fraction_leaf'],\n",
    "                         max_features=params['max_features'],\n",
    "                         bootstrap = params['bootstrap'],\n",
    "                         oob_score= params['oob_score'],\n",
    "                         n_jobs=1,\n",
    "                         random_state=fold) for _ in range(num_algorithms)]\n",
    "\n",
    "for alg_id in range(num_algorithms):\n",
    "    # prepare survival forest dataset and split the data accordingly\n",
    "    X_train, Y_train = construct_dataset_for_algorithm_id(features, performances, alg_id, algorithm_cutoff_time)            \n",
    "    X_train = imputer[alg_id].fit_transform(features)\n",
    "    X_train = scaler[alg_id].fit_transform(X_train)\n",
    "    models[alg_id].fit(X_train, Y_train)\n",
    "\n",
    "    \n",
    "##### Predict Survival Functions and respective Risks #####\n",
    "instance_id = 6\n",
    "features = test_scenario.feature_data.to_numpy()[instance_id]\n",
    "\n",
    "event_times = []\n",
    "survival_functions = []\n",
    "\n",
    "for alg_id in range(num_algorithms):\n",
    "    X_test = np.reshape(features, (1, -1))\n",
    "    X_test = imputer[alg_id].transform(X_test)\n",
    "    X_test = scaler[alg_id].transform(X_test)\n",
    "    event_times.append(models[alg_id].event_times_)\n",
    "    survival_functions.append(models[alg_id].predict_survival_function(X_test)[0])\n",
    "\n",
    "for alg_id in range(num_algorithms):\n",
    "    event_times[alg_id] = np.append(0.0, event_times[alg_id])\n",
    "    event_times[alg_id] = np.append(event_times[alg_id], algorithm_cutoff_time)\n",
    "    survival_functions[alg_id] = np.append(1.0, survival_functions[alg_id])\n",
    "    \n",
    "    # Repeat last survival probability for plot\n",
    "    survival_functions[alg_id] = np.append(survival_functions[alg_id], survival_functions[alg_id][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions, Survival Functions & Event Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_id = 6\n",
    "features = test_scenario.feature_data.to_numpy()[instance_id]\n",
    "\n",
    "event_times = []\n",
    "survival_functions = []\n",
    "\n",
    "for alg_id in range(num_algorithms):\n",
    "    X_test = np.reshape(features, (1, -1))\n",
    "    X_test = imputer[alg_id].transform(X_test)\n",
    "    X_test = scaler[alg_id].transform(X_test)\n",
    "    # print(models[alg_id].event_times_.shape, models[alg_id].predict_survival_function(X_test)[0].x.shape)\n",
    "    # print(models[alg_id].predict_survival_function(X_test)[0].y)\n",
    "    event_times.append(models[alg_id].event_times_)\n",
    "    survival_functions.append(models[alg_id].predict_survival_function(X_test)[0])   # Added the y\n",
    "    \n",
    "for alg_id in range(num_algorithms):\n",
    "    event_times[alg_id] = np.append(0.0, event_times[alg_id])\n",
    "    event_times[alg_id] = np.append(event_times[alg_id], algorithm_cutoff_time)\n",
    "    survival_functions[alg_id] = np.append(1.0, survival_functions[alg_id])\n",
    "    \n",
    "    # Repeat last survival probability for plot\n",
    "    survival_functions[alg_id] = np.append(survival_functions[alg_id], survival_functions[alg_id][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival Functions Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure settings\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "ax.ticklabel_format(useOffset=False, style='plain')\n",
    "ax.set_ylabel('Survival Probability')\n",
    "ax.set_xlabel('Normalized Algorithm Runtime')\n",
    "\n",
    "# plot survival functions \n",
    "for alg_id in range(num_algorithms):\n",
    "    # print(alg_id)\n",
    "    events =  event_times[alg_id] / algorithm_cutoff_time\n",
    "    # print(events.shape, survival_functions[alg_id].shape)\n",
    "    sns.lineplot(x=events, y=survival_functions[alg_id], ax=ax, label='Algorithm {}'.format(alg_id))\n",
    "    \n",
    "fig.savefig(r'./survival_functions.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulated Risk Score Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "risk_funcs = [lambda x: x, lambda x: x**2, lambda x: x**5]\n",
    "    \n",
    "for ax_num, risk_func in enumerate(risk_funcs):\n",
    "    axes[ax_num].ticklabel_format(useOffset=False, style='plain')\n",
    "    axes[ax_num].set_ylabel('Truncated Risk Score')\n",
    "    axes[ax_num].set_xlabel('Normalized Algorithm Runtime')\n",
    "    if ax_num == 0:\n",
    "        axes[ax_num].set_title('Risk Function: $\\mathrm{r(x) = x}$')\n",
    "        \n",
    "    elif ax_num == 1:\n",
    "        axes[ax_num].set_title('Risk Function: $\\mathrm{r(x) = x^3}$')\n",
    "\n",
    "    else:\n",
    "        axes[ax_num].set_title('Risk Function: $\\mathrm{r(x) = x^6}$')\n",
    "    \n",
    "    for alg_id in range(num_algorithms):\n",
    "        events = event_times[alg_id] / algorithm_cutoff_time\n",
    "        risk_scores = np.cumsum(survival_functions[alg_id][:-1] * np.diff(risk_func(events)))\n",
    "        sns.lineplot(x=events[1:], y=risk_scores, ax=axes[ax_num], label='Algorithm {}'.format(alg_id))\n",
    "\n",
    "fig.savefig(r'./risk_functions.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulated Risk Score Plots (Algorithm 1 & 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = [1, 3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "risk_funcs = [lambda x: x, lambda x: x**3, lambda x: x**6]\n",
    "    \n",
    "for ax_num, risk_func in enumerate(risk_funcs):\n",
    "    axes[ax_num].ticklabel_format(useOffset=False, style='plain')\n",
    "    axes[ax_num].set_ylabel('Truncated Risk Score')\n",
    "    axes[ax_num].set_xlabel('Normalized Algorithm Runtime')\n",
    "    if ax_num == 0:\n",
    "        axes[ax_num].set_title('Risk Function: $\\mathrm{r(x) = x}$')\n",
    "        \n",
    "    elif ax_num == 1:\n",
    "        axes[ax_num].set_title('Risk Function: $\\mathrm{r(x) = x^3}$')\n",
    "\n",
    "    else:\n",
    "        axes[ax_num].set_title('Risk Function: $\\mathrm{r(x) = x^6}$')\n",
    "    \n",
    "    for alg_id in subset:\n",
    "        events = event_times[alg_id] / algorithm_cutoff_time\n",
    "        risk_scores = np.cumsum(survival_functions[alg_id][:-1] * np.diff(risk_func(events)))\n",
    "        sns.lineplot(x=events[1:], y=risk_scores, ax=axes[ax_num], label='Algorithm {}'.format(alg_id))\n",
    "        \n",
    "fig.savefig(r'./subset_risk_functions.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulated Risk Score Difference (Algorithm 1 & 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modification of Survival Functions in order to establish a clear example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_functions[1][-25:-23] = 0.20733033\n",
    "survival_functions[1][-23:-20] = 0.18733033\n",
    "survival_functions[1][-20:-18] = 0.1733033\n",
    "survival_functions[1][-19:-16] = 0.12192181\n",
    "survival_functions[1][-16:-10] = 0.080192181\n",
    "survival_functions[1][-10:-7] = 0.05437217\n",
    "survival_functions[1][-7:-4] = 0.03867688\n",
    "survival_functions[1][-4:] = 0.0247688"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set figure settings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "\n",
    "### Survival Plot ###\n",
    "axes[0].ticklabel_format(useOffset=False, style='plain')\n",
    "axes[0].set_ylabel('Survival Probability', fontsize=14)\n",
    "axes[0].set_xlabel('Normalized Algorithm Runtime', fontsize=14)\n",
    "\n",
    "colors = [\"windows blue\", \"amber\", \"greyish\", \"faded green\", \"dusty purple\"]\n",
    "palette = sns.xkcd_palette(colors)\n",
    "\n",
    "# plot survival functions \n",
    "for alg_id in range(num_algorithms):\n",
    "    events =  event_times[alg_id] / algorithm_cutoff_time\n",
    "    sns.lineplot(x=events, y=survival_functions[alg_id], ax=axes[0], label='Algorithm {}'.format(alg_id), color=palette[alg_id])\n",
    "    \n",
    "    \n",
    "### Cumulated Risk Score Difference Plot ###\n",
    "axes[1].set_ylim([-0.075, 0.1])\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "\n",
    "\n",
    "subset = [1, 3]\n",
    "alg_0 = subset[0]\n",
    "alg_1 = subset[1]\n",
    "\n",
    "axes[1].ticklabel_format(useOffset=False, style='plain')\n",
    "axes[1].set_ylabel('Truncated Risk Score Difference',  fontsize=14)\n",
    "axes[1].set_xlabel('Normalized Algorithm Runtime', fontsize=14)\n",
    "\n",
    "# compute all events occuring for algorithm 1 and algorithm 3\n",
    "overall_events = set(event for alg_id in subset for event in event_times[alg_id].tolist())\n",
    "overall_events.update([0.0, algorithm_cutoff_time])\n",
    "overall_events = sorted(overall_events)\n",
    "\n",
    "# compute all respective differences regarding the algorithms' survival probabilities\n",
    "difference = np.zeros(len(overall_events))\n",
    "for event_idx, event in enumerate(overall_events):\n",
    "    # get index of respective survival probability for each algorithm\n",
    "    idx = np.zeros(len(subset), dtype=int)\n",
    "    for num, alg_id in enumerate(subset):\n",
    "        idx[num] = np.argmax(event_times[alg_id] >= event)\n",
    "\n",
    "    difference[event_idx] = survival_functions[alg_0][idx[0]] - survival_functions[alg_1][idx[1]]\n",
    "    \n",
    "overall_events = np.asarray(overall_events) / algorithm_cutoff_time\n",
    "\n",
    "y1 = np.cumsum(difference[1:] * np.diff(overall_events**1.0))\n",
    "y2 = np.cumsum(difference[1:] * np.diff(overall_events**2.0))\n",
    "y3 = np.cumsum(difference[1:] * np.diff(overall_events**4.0)) \n",
    "y4 = np.cumsum(difference[1:] * np.diff(overall_events**6.0))\n",
    "\n",
    "palette = sns.color_palette(\"Blues\", 10)\n",
    "\n",
    "sns.lineplot(x=overall_events[1:], y=y1, ax=axes[1], label='$E[T_1^1] - E[T_3^1]$', color=palette[3])\n",
    "sns.lineplot(x=overall_events[1:], y=y2, ax=axes[1], label='$E[T_1^2] - E[T_3^2]$', color=palette[5])\n",
    "sns.lineplot(x=overall_events[1:], y=y3, ax=axes[1], label='$E[T_1^4] - E[T_3^4]$', color=palette[7])\n",
    "sns.lineplot(x=overall_events[1:], y=y4, ax=axes[1], label='$E[T_1^6] - E[T_3^6]$', color=palette[9])\n",
    "\n",
    "#fb1 = axes[1].fill_between([0.0, 1.0], y1=[0.0, 0.0], y2=[0.1, 0.1], facecolor=\"#f03838\", zorder=0, alpha=0.3)\n",
    "fb1 = axes[1].fill_between([0.0, 1.0], y1=[0.0, 0.0], y2=[0.1, 0.1], facecolor=\"#9ACD32\", zorder=0, alpha=0.3)\n",
    "fb1.set_hatch('//')\n",
    "\n",
    "fb2 = axes[1].fill_between([0.0, 1.0], y1=[0.0, 0.0], y2=[-2.0, -2.0], facecolor=\"#fab75d\", zorder=0, alpha=0.3)\n",
    "fb2.set_hatch(\"\\\\\\\\\")\n",
    "\n",
    "fig.savefig(r'./joint_plot.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_1 = 1\n",
    "alg_2 = 3\n",
    "\n",
    "diffs_poly = []\n",
    "diffs_exp = []\n",
    "alpha_range_poly = np.linspace(1,6,10000)\n",
    "alpha_range_exp = np.linspace(0,1,10000)\n",
    "\n",
    "for alpha in alpha_range_poly:\n",
    "    risk_func_poly = lambda x: x**alpha\n",
    "    \n",
    "    events_1 = event_times[alg_1] / algorithm_cutoff_time\n",
    "    events_2 = event_times[alg_2] / algorithm_cutoff_time\n",
    "    \n",
    "    risk_score_1_poly = np.sum(survival_functions[alg_1][:-1] * np.diff(risk_func_poly(events_1)))\n",
    "    risk_score_2_poly = np.sum(survival_functions[alg_2][:-1] * np.diff(risk_func_poly(events_2)))\n",
    "    \n",
    "    diffs_poly.append(risk_score_1_poly - risk_score_2_poly)\n",
    "    \n",
    "for alpha in alpha_range_exp:\n",
    "    exp_thresh = 1\n",
    "    risk_func_exp =  lambda x: np.minimum((-1) * alpha * np.log(1.0 - x), exp_thresh)\n",
    "\n",
    "    events_1 = event_times[alg_1] / algorithm_cutoff_time\n",
    "    events_2 = event_times[alg_2] / algorithm_cutoff_time\n",
    "\n",
    "    risk_score_1_exp = np.sum(survival_functions[alg_1][:-1] * np.diff(risk_func_exp(events_1)))\n",
    "    risk_score_2_exp = np.sum(survival_functions[alg_2][:-1] * np.diff(risk_func_exp(events_2)))\n",
    "\n",
    "    diffs_exp.append(risk_score_1_exp - risk_score_2_exp)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=alpha_range_poly, ax=ax, y=diffs_poly, label='polynomial')\n",
    "sns.lineplot(x=alpha_range_exp, ax=ax, y=diffs_exp, label='exponential')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Risk Score Difference',  fontsize=14)\n",
    "ax.set_xlabel(r'$ \\alpha$', fontsize=14)\n",
    "\n",
    "ax.set_ylim([-0.1, 0.175])\n",
    "ax.set_xlim([0.0, 6.0])\n",
    "\n",
    "fb1 = ax.fill_between([0.0, 6.0], y1=[0.0, 0.0], y2=[0.175, 0.175], facecolor=\"#9ACD32\", zorder=0, alpha=0.3)\n",
    "fb1.set_hatch('//')\n",
    "\n",
    "fb2 = ax.fill_between([0.0, 6.0], y1=[0.0, 0.0], y2=[-0.125, -0.125], facecolor=\"#fab75d\", zorder=0, alpha=0.3)\n",
    "fb2.set_hatch(\"\\\\\\\\\")\n",
    "fig.savefig(r'./risk_score_lineplot.pdf', bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Schmee & Hahn for PerAlgorithmRegressor\n",
    "schmee_data = pd.read_csv(r'./schmee_normalized_par10.csv')\n",
    "schmee_data.columns = ['ASlib Scenarios', 'Algorithm Selector', 'metric', 'normalized', 'num_results']\n",
    "schmee_data = schmee_data[schmee_data['Algorithm Selector'] == 'ImputedPerAlgorithmRegressor']\n",
    "schmee_data = schmee_data.set_index(keys='ASlib Scenarios', drop=True)\n",
    "schmee_data = schmee_data['normalized']\n",
    "\n",
    "# Load setting comparison data\n",
    "data = pd.read_csv(r'./setting_comparison_normalized_par10.csv')\n",
    "data.columns = ['ASlib Scenario', 'Algorithm Selector', 'Metric', 'PAR10', 'Runtime', 'Ignored']\n",
    "\n",
    "approaches = ['PerAlgorithmRegressor', 'ISAC*like', 'SATzilla\\'11*like', 'SUNNY*like']\n",
    "fig = plt.figure(figsize=(12, 10), constrained_layout=False)\n",
    "num_cols = len(approaches) + 1\n",
    "width_ratios = [30 for _ in range(len(approaches))]\n",
    "width_ratios[0] = 40\n",
    "width_ratios.append(5)\n",
    "\n",
    "gs1 = gridspec.GridSpec(4, num_cols, width_ratios=width_ratios, height_ratios=[100, 5, 5, 5], figure=fig, wspace=0.1, hspace=0.05)\n",
    "gs2 = gridspec.GridSpec(4, num_cols, width_ratios=width_ratios, height_ratios=[100, 5, 5, 5], figure=fig, wspace=0.1, hspace=0.05)\n",
    "\n",
    "axes1 = [fig.add_subplot(gs1[0, num]) for num in range(num_cols)]\n",
    "\n",
    "\n",
    "axes2 = [fig.add_subplot(gs2[1, num]) for num in range(num_cols - 1)]\n",
    "axes3 = [fig.add_subplot(gs2[2, num]) for num in range(num_cols - 1)]\n",
    "axes4 = [fig.add_subplot(gs2[3, num]) for num in range(num_cols - 1)]\n",
    "\n",
    "palette = sns.color_palette(['#FAFAFA', '#F2F2F2', '#EBEBEB', '#E3E3E3', '#DBDBDB', '#D6D6D6', '#D4D4D4', '#CCCCCC', '#C7C7C7', '#BFBFBF', '#BDBDBD', '#B5B5B5', '#ADADAD', '#A9A9A9', '#A6A6A6', '#9E9E9E', '#969696', '#8F8F8F', '#878787', '#808080'], n_colors=20)\n",
    "bottom_palette =sns.color_palette(\"Blues\", n_colors=100)\n",
    "vmin, vmax = 0.0, 5.0\n",
    "\n",
    "for num, approach in tqdm(enumerate(approaches)):\n",
    "    tmp_data = data.copy()\n",
    "    tmp_data = tmp_data[tmp_data['Algorithm Selector'] == approach]\n",
    "    columns = ['PAR10', 'Runtime', 'Ignored']\n",
    "    \n",
    "    if approach == 'PerAlgorithmRegressor':\n",
    "        tmp_data = tmp_data.set_index('ASlib Scenario', drop=False)\n",
    "        tmp_data = pd.concat([tmp_data, schmee_data], axis=1, sort=True)\n",
    "        columns.extend(['S & H'])\n",
    "        tmp_col = ['ASlib Scenario','Algorithm Selector', 'Metric']\n",
    "        tmp_col.extend(columns)\n",
    "        tmp_data.columns = tmp_col\n",
    "\n",
    "    tmp_data = tmp_data.pivot('ASlib Scenario','Algorithm Selector', columns)\n",
    "    tmp_data.columns = columns\n",
    "    \n",
    "    if num + 1 < len(approaches):\n",
    "        sns.heatmap(tmp_data, cmap=palette, linewidths=.5, ax=axes1[num], annot=True, cbar_kws={'label': 'Normalized PAR10'}, vmax=vmax, vmin=vmin, cbar=False)\n",
    "    \n",
    "    else:\n",
    "        sns.heatmap(tmp_data, cmap=palette, linewidths=.5, ax=axes1[num], annot=True, cbar_kws={'label': 'Normalized PAR10'}, vmax=vmax, vmin=vmin, cbar_ax=axes1[-1])\n",
    "\n",
    "    axes1[num].tick_params(axis='both', which='major', labelsize=10, labelbottom=False, bottom=False, top=False, labeltop=True)\n",
    "    labels = axes1[num].get_xticklabels()\n",
    "    axes1[num].set_xticklabels(labels, rotation=45, ha='left')\n",
    "    axes1[num].set_xlabel('')\n",
    "    axes1[num].set_ylabel('')    \n",
    "    \n",
    "    tmp = np.asarray(tmp_data.values, dtype=float)\n",
    "    median = np.median(tmp, axis=0)    \n",
    "    median = median.reshape(1, median.size)\n",
    "    mean = np.mean(tmp, axis=0)\n",
    "    mean = mean.reshape(1, mean.size)\n",
    "    ranks = rankdata(tmp, axis=1)\n",
    "    ranks = np.mean(ranks, axis=0)\n",
    "    ranks = ranks.reshape(1, ranks.size)\n",
    "\n",
    "    sns.heatmap(median, ax=axes2[num], annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=0.0, vmax=vmax)\n",
    "    sns.heatmap(mean, ax=axes3[num], annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=0.0, vmax=vmax)\n",
    "    sns.heatmap(ranks, ax=axes4[num], annot=True, cbar=False, xticklabels=False, yticklabels=False, cmap=bottom_palette, vmin=0.0, vmax=vmax)\n",
    "    \n",
    "    num_scenarios, num_settings = tmp.shape\n",
    "    for scenario_id in range(num_scenarios):\n",
    "        tmp_texts = axes1[num].texts[scenario_id * num_settings: scenario_id* num_settings + num_settings]\n",
    "        min_val = min([float(text.get_text()) for text in tmp_texts])\n",
    "        texts = [text for text in tmp_texts if float(text.get_text()) == min_val]\n",
    "        \n",
    "        for text in texts:\n",
    "            #text.set_size(12)\n",
    "            text.set_weight('bold')\n",
    "            text.set_color('black')\n",
    "            text.set_style('italic')\n",
    "\n",
    "    \n",
    "    for ax in [axes2[num], axes3[num], axes4[num]]:\n",
    "        min_val = min([float(text.get_text()) for text in ax.texts])\n",
    "        indices = [num for num, text in enumerate(ax.texts) if float(text.get_text()) == min_val]\n",
    "\n",
    "        for idx in indices:\n",
    "            text = ax.texts[idx]\n",
    "            #text.set_size(12)\n",
    "            text.set_weight('bold')\n",
    "            text.set_color('black')\n",
    "            text.set_style('italic')\n",
    "    \n",
    "    \n",
    "    if num > 0:\n",
    "        axes1[num].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "#for ax in axes1[:-1]:\n",
    "#    bottom, top = ax.get_ylim()\n",
    "#    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    \n",
    "for num, ax in enumerate(axes2):\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "axes2[0].set_ylabel('Median', rotation=0, labelpad=30)\n",
    "    \n",
    "for ax in axes3:\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "axes3[0].set_ylabel('Mean', rotation=0, labelpad=30)\n",
    "\n",
    "for num, ax in enumerate(axes4):\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel(approaches[num], y=1.3, fontsize=14)\n",
    "\n",
    "axes4[0].set_ylabel('Mean Rank', rotation=0, labelpad=30)\n",
    "\n",
    "fig.savefig(r'./setting_comparison.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsolved Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_data = pd.read_csv(r'./unsolved_final.csv')\n",
    "\n",
    "\n",
    "name_map = {'Expectation_algorithm_survival_forest':'ExpectationSurvivalForest', \n",
    "            'SurrogateAutoSurvivalForest':'RiskAverseSurvivalForest',\n",
    "           'GridSearch_algorithm_survival_forest':'GridSearchSurvivalForest',\n",
    "           'PAR10_algorithm_survival_forest':'PAR10SurvivalForest',\n",
    "           'isac':'ISAC*like\\n(PAR10)',\n",
    "           'multiclass_algorithm_selector':'MultiClassSelector',\n",
    "           'per_algorithm_regressor':'PerAlgorithmRegressor\\n(Runtime)',\n",
    "           'satzilla-11':'SATzilla\\'11*like\\n(Runtime)',\n",
    "           'snnap':'SNNAP*like',\n",
    "           'sunny':'SUNNY*like\\n(Runtime)'}\n",
    "\n",
    "delete = ['MultiClassSelector', 'SNNAP*like', 'GridSearchSurvivalForest', 'sbs','ISAC*like\\n(PAR10)', 'PerAlgorithmRegressor\\n(Runtime)', 'SATzilla\\'11*like\\n(Runtime)', 'SUNNY*like\\n(Runtime)']\n",
    "\n",
    "survival_data = survival_data.replace(name_map)\n",
    "survival_data = survival_data[~survival_data['Algorithm Selector'].isin(delete)]\n",
    "# survival_data = survival_data.drop('metric', axis=1)\n",
    "survival_data.to_csv(r'./unsolved_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For each baseline select results from the overall best setting (PAR10, Runtime or Ignored) in terms of median performance aggregated over all scenarios \n",
    "\n",
    "# get data for own approaches from unsolved instances table\n",
    "survival_data = pd.read_csv(r'./unsolved_final.csv')\n",
    "survival_data = survival_data.pivot('ASlib Scenario','Algorithm Selector','unsolved')\n",
    "survival_data = survival_data[['ExpectationSurvivalForest', 'PAR10SurvivalForest', 'RiskAverseSurvivalForest']]\n",
    "\n",
    "# get data for baselines from the comparison unsolved instances (best value from different settings)\n",
    "baseline_data =  pd.read_csv(r'./setting_comparison_unsolved.csv')\n",
    "min_median_setting = baseline_data.groupby(['Algorithm Selector', 'Metric']).median().idxmin(axis=1)\n",
    "min_median_setting = min_median_setting.droplevel(level=1)\n",
    "min_median_setting = list(zip(min_median_setting.index, min_median_setting))\n",
    "baseline_map = dict(min_median_setting).copy()\n",
    "baseline_data = baseline_data.pivot('ASlib Scenario', 'Algorithm Selector', ['PAR10', 'Runtime', 'Ignored']).swaplevel(axis=1).sort_index(1)\n",
    "baseline_data = baseline_data[min_median_setting]\n",
    "baseline_data = baseline_data.droplevel(axis=1, level=1)\n",
    "\n",
    "# add Schmee & Hahn for PerAlgorithmRegressor and recompute min median for unsolved instances metric\n",
    "schmee_data = pd.read_csv(r'./schmee_unsolved.csv')\n",
    "schmee_data = schmee_data[schmee_data['Algorithm Selector'] == 'ImputedPerAlgorithmRegressor']\n",
    "schmee_data = schmee_data.set_index(keys='ASlib Scenario', drop=True)\n",
    "\n",
    "if schmee_data['unsolved'].median() < baseline_data['PerAlgorithmRegressor'].median():\n",
    "    baseline_data['PerAlgorithmRegressor'] = schmee_data['normalized']\n",
    "    baseline_map['PerAlgorithmRegressor'] = 'S & H'\n",
    "    \n",
    "# adjust ordering of algorithms and rename according to best setting\n",
    "baseline_data = baseline_data[['PerAlgorithmRegressor', 'MultiClassSelector', 'ISAC*like', 'SATzilla\\'11*like', 'SUNNY*like']]\n",
    "\n",
    "data = pd.concat([survival_data, baseline_data], axis = 1).T.drop_duplicates().T\n",
    "baseline_index = [''.join([selector, '\\n(' + baseline_map[selector] + ')']) for selector in data.columns if selector in baseline_map.keys()]\n",
    "data = data.rename(columns={selector: ''.join([selector, '\\n(' + setting + ')']) for selector, setting in baseline_map.items()})\n",
    "\n",
    "data = data*100\n",
    "data = data.round(1)\n",
    "data.to_latex('median_unsolved_table.tex')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASlib Scenario overview table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aslib_scenario.aslib_scenario import ASlibScenario\n",
    "\n",
    "aslib_table = pd.DataFrame(columns=['Scenario', '#I', '#U', '#A', '#F', 'T', '%C'])\n",
    "\n",
    "scenarios = ['ASP-POTASSCO', 'BNSL-2016', 'CPMP-2015', 'CSP-2010',\n",
    "       'CSP-MZN-2013', 'CSP-Minizinc-Time-2016', 'GRAPHS-2015',\n",
    "       'MAXSAT-PMS-2016', 'MAXSAT-WPMS-2016', 'MAXSAT12-PMS',\n",
    "       'MAXSAT15-PMS-INDU', 'MIP-2016', 'PROTEUS-2014', 'QBF-2011',\n",
    "       'QBF-2014', 'QBF-2016', 'SAT03-16_INDU', 'SAT11-HAND',\n",
    "       'SAT11-INDU', 'SAT11-RAND', 'SAT12-ALL', 'SAT12-HAND',\n",
    "       'SAT12-INDU', 'SAT12-RAND', 'SAT15-INDU', 'TSP-LION2015']\n",
    "\n",
    "for num, scenario_name in enumerate(scenarios):\n",
    "    scenario = ASlibScenario()\n",
    "    scenario.read_scenario(Path('./workspaces/aslib/' + scenario_name))\n",
    "    \n",
    "    name = str(scenario.scenario)\n",
    "    instances = len(scenario.instances)\n",
    "    algorithms = len(scenario.algorithms)\n",
    "    features = len(scenario.feature_data.columns)\n",
    "    timeout = scenario.algorithm_cutoff_time\n",
    "    \n",
    "    performance_data = scenario.performance_data.values\n",
    "    censored = (np.sum(performance_data >= timeout) / performance_data.size) * 100\n",
    "    unsolved = np.sum(np.all((performance_data >= timeout), axis=1))\n",
    "    \n",
    "    aslib_table.loc[num] = [name, instances, unsolved, algorithms, features, timeout, censored]\n",
    "    \n",
    "aslib_table = aslib_table.round(1)\n",
    "aslib_table = aslib_table.set_index('Scenario', drop=True)\n",
    "aslib_table = aslib_table.T\n",
    "aslib_table.to_latex('aslib_table.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aslib_scenario.aslib_scenario import ASlibScenario\n",
    "\n",
    "aslib_table = pd.DataFrame(columns=['Scenario', '#I', '#U', '#A', '#F', 'T', '%C'])\n",
    "\n",
    "scenarios = ['ASP-POTASSCO', 'BNSL-2016', 'CPMP-2015', 'CSP-2010',\n",
    "       'CSP-MZN-2013', 'CSP-Minizinc-Time-2016', 'GRAPHS-2015',\n",
    "       'MAXSAT-PMS-2016', 'MAXSAT-WPMS-2016', 'MAXSAT12-PMS',\n",
    "       'MAXSAT15-PMS-INDU', 'MIP-2016', 'PROTEUS-2014', 'QBF-2011',\n",
    "       'QBF-2014', 'QBF-2016', 'SAT03-16_INDU', 'SAT11-HAND',\n",
    "       'SAT11-INDU', 'SAT11-RAND', 'SAT12-ALL', 'SAT12-HAND',\n",
    "       'SAT12-INDU', 'SAT12-RAND', 'SAT15-INDU', 'TSP-LION2015']\n",
    "\n",
    "for num, scenario_name in enumerate(scenarios):\n",
    "    scenario = ASlibScenario()\n",
    "    scenario.read_scenario(Path('./workspaces/aslib/' + scenario_name))\n",
    "    \n",
    "    name = str(scenario.scenario)\n",
    "    instances = len(scenario.instances)\n",
    "    algorithms = len(scenario.algorithms)\n",
    "    features = len(scenario.feature_data.columns)\n",
    "    timeout = scenario.algorithm_cutoff_time\n",
    "    \n",
    "    performance_data = scenario.performance_data.values\n",
    "    censored = (np.sum(performance_data >= timeout) / performance_data.size) * 100\n",
    "    unsolved = np.sum(np.all((performance_data >= timeout), axis=1))\n",
    "    \n",
    "    aslib_table.loc[num] = [name, instances, unsolved, algorithms, features, timeout, censored]\n",
    "    \n",
    "aslib_table = aslib_table.round(1)\n",
    "aslib_table = aslib_table.set_index('Scenario', drop=True)\n",
    "aslib_table = aslib_table.T\n",
    "aslib_table.to_latex('aslib_table.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import os\n",
    "\n",
    "for num, scenario_name in enumerate(scenarios):\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    sys.stderr = open(os.devnull, \"w\")\n",
    "    \n",
    "    scenario = ASlibScenario()\n",
    "    scenario.read_scenario(Path('./workspaces/aslib/' + scenario_name))\n",
    "    \n",
    "    sys.stdout = sys.__stdout__\n",
    "    sys.stderr = sys.__stderr__\n",
    "    \n",
    "    performance_data = scenario.performance_data.values\n",
    "    censored = (np.sum(performance_data >= timeout) / performance_data.size)\n",
    "    per_alg_censored = (np.sum(performance_data >= timeout, axis= 0) / performance_data.size) / censored\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    sizes = per_alg_censored\n",
    "    #cs=cm.Set1(np.arange(sizes.size)/sizes.size, alpha=0.7)\n",
    "\n",
    "    ax.pie(sizes, autopct='%3.0f%%',\n",
    "        shadow=False, startangle=90)\n",
    "    \n",
    "    ax.axis('equal')\n",
    "    ax.set_xlabel(scenario_name)\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(r'./pie_charts/' + scenario_name + '.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
